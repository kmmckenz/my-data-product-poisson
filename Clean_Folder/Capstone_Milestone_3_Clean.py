from flask import Flask, render_template_string
import pandas as pd
import statistics
import numpy as np
import time
#loads packages

# # Help
# 
# ### Purpose:
# To clean data prior to modeling.
# 
# ### How to Run:
# - Run each cell in order (previous generated information may not be captured otherwise).
# - Modify the data and cleaning techniques as needed.
# 
# ### What You'll Get:
# - Prints and descriptions of the data set.
# - Cleaning of missing values, duplicated values, and outliers (options of standard deviations away IQRs away, and log transformation).
# 
# ### Functions Included:
# - pd.read_csv()
# - pd.merge()
# - print ()
# - head()
# - tail()
# - sum()
# - isna()
# - duplicated()
# - dropna()
# - drop_duplicates()
# - pd.to_datetime()
# - mean()
# - stdev()
# - iqr()
# - quantile()
# - pd.DataFrame()
# - to_csv()
# 
# Please use the `help(function_name)` in a new cell to learn more about any functions.
# 
# Additional note: The interpretations are based on the results last generated by the developer. Any modifications to the data or the process will change the results, and will require new interpretations to be developed.

log_content = []  # Create a global list to store log content

def log_step(step_name):

    try:
        timestamp = time.strftime('%Y-%m-%d %H:%M:%S')
        log_message = f"[{timestamp}] {step_name}"
        log_content.append(log_message)  
        return log_message
    
    except Exception as e:
        log_message = f"An unexpected error occurred: {e}"
        log_content.append(log_message)
        return log_message
# creates function to log steps


def load_data():

    try:
        log_step("Starting to load data")
        data_vaccine = pd.read_csv("/workspaces/my-data-product-poisson/Data/daily-covid-19-vaccine-doses-administered.csv")
        data_deaths = pd.read_csv("/workspaces/my-data-product-poisson/Data/daily-new-confirmed-covid-19-deaths-per-million-people.csv")
        data = pd.merge(data_vaccine, data_deaths, on=['Entity', 'Day'], how='inner')
        log_step("Data loaded successfully")
        return data
    
    except FileNotFoundError as e:
        log_step(f"File not found error: {e}")
        return None
    
    except Exception as e:
        log_step(f"An unexpected error occurred: {e}")
        return None
    
# Function to load data from CSV or JSON URL
#def load_data(url):
#    try:
#        if url.endswith('.csv'):
#            df = pd.read_csv(url)
#            return df, "CSV loaded successfully."
#        elif url.endswith('.json'):
#            df = pd.read_json(url)
#            return df, "JSON loaded successfully."
#        else:
#            return None, "Unsupported file type. Please enter a .csv or .json URL."
#    except Exception as e:
#        return None, f"An error occurred: {str(e)}"
# creates function for loading original data


def about_data():
    try:
        content = """Data was obtained from Our World in Data, a site that 
        aggregated information from the World Health Organization 
        (WHO) and a multitude of population details from various 
        sources (Appel et al., 2025). As WHO is a leader in the 
        research on the COVID-19 pandemic, this source is appropriate 
        for developing the product. The data contains 67,140 observations 
        on the entity, day, number of COVID-19 doses administered  
        (daily), and daily new confirmed deaths due to COVID-19.
        It was originally hoped that the data set would contain more data. However,
        upon inspection of the other data sets available, the larger data sets contained
        cumulative data and count data, which would eliminate the possibility of
        performing a Poisson regression (Appel et al., 2025). Therefore, the data sets on
        the counts of administered vaccines and COVID-19 related deaths were
        combined. This was performed using the pd.merge() function, where the entity
        and the day were matched within each data set to maintain consistency.
        Information on booster doses administered would have caused missing values
        within the data, so it was not added"""
        return content
    except Exception as e:
        log_step(f"An unexpected error occurred: {e}")
        return None
    # returns error
# prints message for user to read
    
def scrub_data(data):
    try:
        log_step("Starting to scrub data")
        
        data['Daily new confirmed deaths due to COVID-19'] = \
            data["Daily new confirmed deaths due to COVID-19 per million people"] * 1000000
        print("Data transformed to individuals instead of per million.")
        # Convert 'Daily new confirmed deaths due to COVID-19 per million people' to individual counts

        data['Day'] = pd.to_datetime(data['Day'])
        print("Date column converted to datetime format.")
        # Convert 'Day' column to datetime format
        
        if data.isnull().values.sum() != 0:
            data = data.dropna()
            log_step("N/A values removed")   
            print("N/A values removed from data set.")
        else:
            print("No N/A values found in data set.")
        # Handle missing values
        
        if data.duplicated().any():
            data = data.drop_duplicates()
            log_step("Duplicated values removed")   
            print("Duplicated values removed from data set.")
        else:
            print("No duplicated values found in data set.")
        # Handle duplicated values

        print(data.dtypes)
        # prints dtypes
        log_step("Data scrubbed successfully")
        return data 
        # Returns the cleaned data
    
    except Exception as e:
        log_step(f"An unexpected error occurred while scrubbing the data: {e}")
        return None
    # returns error

def outliers(data):
    try:
        log_step("Starting outlier handling")
        
        std_high_vaccine = statistics.mean(data['COVID-19 doses (daily)']) + (statistics.stdev(data['COVID-19 doses (daily)']) * 3)
        std_low_vaccine = statistics.mean(data['COVID-19 doses (daily)']) - (statistics.stdev(data['COVID-19 doses (daily)']) * 3)
        std_high_death = statistics.mean(data['Daily new confirmed deaths due to COVID-19']) + (statistics.stdev(data['Daily new confirmed deaths due to COVID-19']) * 3)
        std_low_death = statistics.mean(data['Daily new confirmed deaths due to COVID-19']) - (statistics.stdev(data['Daily new confirmed deaths due to COVID-19']) * 3)
        # creates cut offs for outliers through standard deviations

        Q1_vaccine = np.quantile(data['COVID-19 doses (daily)'], 0.25)
        Q3_vaccine = np.quantile(data['COVID-19 doses (daily)'], 0.75)
        IQR_vaccine = Q3_vaccine - Q1_vaccine
        iqr_high_vaccine = Q3_vaccine + (IQR_vaccine * 1.5)
        iqr_low_vaccine = Q1_vaccine - (IQR_vaccine * 1.5)
        Q1_death = np.quantile(data['Daily new confirmed deaths due to COVID-19'], 0.25)
        Q3_death = np.quantile(data['Daily new confirmed deaths due to COVID-19'], 0.75)
        IQR_death = Q3_death - Q1_death
        iqr_high_death = Q3_death + (IQR_death * 1.5)
        iqr_low_death = Q1_death - (IQR_death * 1.5)
        # creates cut offs for outliers through iqr

        data_nomean_out_vaccine = []
        data_nomean_out_death = []
        data_nomedian_out_vaccine = []
        data_nomedian_out_death = []
        data_log_vaccine = []
        data_log_death = []
        # creates empty lists

        for value in data['COVID-19 doses (daily)']:
            if value > std_high_vaccine or value < std_low_vaccine:
                clipped_value = max(min(value, std_high_vaccine), std_low_vaccine)
                data_nomean_out_vaccine.append(clipped_value)
            else:
                data_nomean_out_vaccine.append(value)
        # handles outliers by standard deviations for vaccine

        for value in data['Daily new confirmed deaths due to COVID-19']:
            if value > std_high_death or value < std_low_death:
                clipped_value = max(min(value, std_high_death), std_low_death)
                data_nomean_out_death.append(clipped_value)
            else:
                data_nomean_out_death.append(value)
        # handles outliers by standard deviations for death

        for value in data['COVID-19 doses (daily)']:
            if value > iqr_high_vaccine or value < iqr_low_vaccine:
                clipped_value = max(min(value, iqr_high_vaccine), iqr_low_vaccine)
                data_nomedian_out_vaccine.append(clipped_value)
            else:
                data_nomedian_out_vaccine.append(value)
        # handles outliers by iqrs for vaccine

        for value in data['Daily new confirmed deaths due to COVID-19']:
            if value > iqr_high_death or value < iqr_low_death:
                clipped_value = max(min(value, iqr_high_death), iqr_low_death)
                data_nomedian_out_death.append(clipped_value)
            else:
                data_nomedian_out_death.append(value)
        # handles outliers by iqr for death
    
        for value in data['COVID-19 doses (daily)']:
            if value > std_high_vaccine or value < std_low_vaccine or value > iqr_high_vaccine or value < iqr_low_vaccine:
                data_log_vaccine.append(np.log1p(value))  # Log-transform if outlier detected
            else:
                data_log_vaccine.append(value)

        for value in data['Daily new confirmed deaths due to COVID-19']:
            if value > std_high_death or value < std_low_death or value > iqr_high_death or value < iqr_low_death:
                data_log_death.append(np.log1p(value))  # Log-transform if outlier detected
            else:
                data_log_death.append(value)
        # log transforms data

        data_nomean = pd.DataFrame({
            'COVID-19 doses (daily)': data_nomean_out_vaccine,
            'Daily new confirmed deaths due to COVID-19': data_nomean_out_death
        })
        data_nomedian = pd.DataFrame({
            'COVID-19 doses (daily)': data_nomedian_out_vaccine,
            'Daily new confirmed deaths due to COVID-19': data_nomedian_out_death
        })
        data_lg = pd.DataFrame({
            'COVID-19 doses (daily)': data_log_vaccine,
            'Daily new confirmed deaths due to COVID-19': data_log_death
        })
        # creates and fills data frames 
        log_step("Outliers removed successfully")
        return data_nomean, data_nomedian, data_lg

    except Exception as e:
        print(f"Error processing outliers: {e}")
        return None, None, None
    # prints errors

def scrub_new_data(data):
    try:
        log_step("Starting to scrub data")
        
        if data.isnull().values.sum() != 0:
            data = data.dropna()
            log_step("N/A values removed")  
            print("N/A values removed from data set.")
        else:
            print("No N/A values found in data set.")
        # Handle missing values
        
        if data.duplicated().any():
            data = data.drop_duplicates()
            log_step("Duplicated values removed")   
            print("Duplicated values removed from data set.")
        else:
            print("No duplicated values found in data set.")
        # Handle duplicated values

        print(data.dtypes)
        # prints dtypes
        log_step("Data scrubbed successfully")
        return data 
        # Returns the cleaned data
    
    except Exception as e:
        log_step(f"An unexpected error occurred while scrubbing the data: {e}")
        return None
    # returns error

def about_cleaning():
    try:
        content2 = """ Data scrubbing and cleaning involved a conversion, checking for missing values,
        duplicated observations, appropriate data types assigned, and outlier
        examination (National Cancer Institute, 2023). When the data set was uploaded
        and printed, the number of deaths was in per million. This value was converted
        into the original count by multiplying a million and made as a new column in the
        data frame. Using the isnull() with the sum() function, it was shown that there
        were no missing values. The duplicated() and sum() functions were also
        performed to search for duplicated observations, which none were found. As for
        the data type, applying dtypes to the data resulted in entity as an object, day as
        an object, COVID-19 doses (daily) as a float, daily new confirmed deaths due to
        COVID-19 per million people as a float, and daily new confirmed deaths due to
        COVID-19 as a float. All of these are appropriate, with the exception of the day.
        This was converted into date time format using pd.to_datetime().
        With the outliers, four techniques were utilized to compare against. There were
        many found, as 1,247 were found using three standard deviations away from the
        mean and 10,842 were found using one and a half interquartile ranges away
        from the median for the vaccination doses administered. With the number of
        deaths, this was 629 data points outside of three standard deviations away and
        8,404 one and a half interquartile ranges away from the median. Due to the
        large amount of outliers, the first method chosen was to keep them and examine
        the impact they had on the data set. The next two used were to remove outliers
        far away from the mean and median. The final method attempted, which had the
        strongest influence on the data structure, was a log transformation. These methods were all included as differing models may have a higher performance
        with differing outlier handling methods. Literature reviews show that there is not a standard method in outlier handling, therefore, multiple were used to identify which is most appropriate for the current data (Aguinis et al., 2013). All will be compared against one another
        to find the best, given the data set at hand."""
        return content2
    except Exception as e:
        log_step(f"An unexpected error occurred: {e}")
        return None
    # returns error
# prints message for user to read

def save_cleaned_data(data, path):
    try:
        data.to_csv(path)
        log_step(f"Data saved successfully to {path}")
        print(f"Data saved successfully to {path}")
        # saves data as csv
    except Exception as e:
        log_step(f"Error saving data to {path}: {e}")
        print(f"Error saving data to {path}: {e}")
        # gives error

# cleans data





    



